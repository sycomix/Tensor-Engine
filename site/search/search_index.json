{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Tensor Engine \u2014 Documentation Index","text":"<p>Welcome to Tensor Engine. This index includes links to tutorials, examples, and API references.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>README: <code>README.md</code> (entry-level overview, examples, and how-to).</li> <li>Quickstart: <code>docs/quickstart.md</code> \u2014 walk-through for building and installing Python wheels.</li> </ul>"},{"location":"#guides","title":"Guides","text":"<ul> <li>Architecture and developer guide: <code>README.md</code> (developer-focused sections)</li> <li>Performance &amp; Benchmarks: <code>docs/bench_descriptions.md</code></li> <li>SafeTensors &amp; Loading state: <code>docs/kronos-modal-format.md</code> and <code>docs/kronos_integration.md</code></li> </ul>"},{"location":"#release-notes","title":"Release Notes","text":"<p>For release automation and CI instructions, see <code>ci/</code> and <code>scripts/</code> scripts present in the repo.</p>"},{"location":"#contact","title":"Contact","text":"<p>Open issues in GitHub, or request guidance by creating a PR with suggested changes.</p> <p>This index is a minimal navigation page for the <code>tensor_engine</code> docs. Expand as needed.</p>"},{"location":"backend_migration_plan/","title":"Backend Trait Migration Plan","text":"<p>This document outlines a migration plan to implement the <code>Backend</code> trait (defined in <code>src/backend.rs</code>) and to migrate expensive ops in <code>src/ops.rs</code> to use backend implementations.</p> <p>Motivation</p> <ul> <li>Allow optimized kernels for various backends (CPU, OpenBLAS, CUDA, wgpu).</li> <li>Centralize backend selection and detection logic.</li> <li>Keep existing ops as a fallback while enabling optimized paths.</li> </ul> <p>High-level steps</p> <ol> <li>Define trait surface in <code>src/backend.rs</code> (already present). Add higher-level    hooks as needed (e.g., <code>matmul</code>, <code>conv2d</code>, <code>softmax</code>) with consistent signatures.</li> <li>Implement <code>CpuBackend</code> in <code>src/backend.rs</code> which delegates to existing ops but    provides optimized versions where available (e.g., OpenBLAS).</li> <li>For ops that can be accelerated (e.g., MatMul, Conv), provide <code>backend.matmul</code>    call site in <code>src/ops.rs</code> and return <code>Option&lt;ArrayD&lt;f32&gt;&gt;</code> when backend    can compute a <code>Some(result)</code>.</li> <li>Move optimized CPU/OpenBLAS detection into <code>backend</code> implementations, not    spread across <code>ops.rs</code>.</li> <li>Add unit tests to ensure consistent forward/backward results across backends.</li> </ol> <p>Notes &amp; Considerations</p> <ul> <li>Maintain existing <code>ops</code> implementations as a fallback to ensure correctness.</li> <li>Begin migration with a single op (MatMul) and expand progressively.</li> <li>Add configuration and auto-detection for backends and a debug logging flag   to force a backend.</li> <li>Ensure thread-safety for backends \u2014 <code>Arc&lt;dyn Backend&gt;</code>, and safe locking.</li> </ul> <p>Tracking</p> <ul> <li>Create a GitHub issue titled \"Migrate ops to Backend trait\" and link here.</li> <li>Add <code>docs/backend_migration_plan.md</code> entry to the project docs and track   progress via PRs.</li> </ul> <p>If you want, I can start implementing the MatMul backend migration as the next step with tests and a small benchmark to validate results.</p>"},{"location":"backend_plan/","title":"Backend plan for tensor_engine","text":"<p>This document outlines a plan to support backend abstraction for CPU and GPU (wgpu/cudarc) and how to structure kernels.</p> <p>Goals:</p> <ul> <li>Provide a minimal CPU backend that uses ndarray / matrixmultiply / OpenBLAS. Keep current code path as default.</li> <li>Introduce a Backend trait describing ops that can be implemented for specific kernels (matmul, conv, softmax,   layernorm, etc.).</li> <li>Implement a <code>cpu</code> backend that uses existing ops as a reference.</li> <li>Implement a <code>wgpu</code> backend first for WebGPU support and memory-safe GPU paths.</li> <li>Implement a <code>cudarc</code> backend for CUDA accelerated kernels; optional feature.</li> </ul> <p>Suggested API sketch:</p> <ul> <li>trait Backend {</li> <li>fn matmul(&amp;self, a: &amp;Tensor, b: &amp;Tensor) -&gt; Tensor;</li> <li>fn conv2d(&amp;self, input: &amp;Tensor, weight: &amp;Tensor) -&gt; Tensor;</li> <li> <p>// other operations   -}</p> </li> <li> <p>Provide a global runtime or per-thread selected backend via <code>BackendRegistry</code> or a context object passed into ops.</p> </li> <li>For now, implement the CPU backend and plugin-style modules that register with the runtime at startup.</li> </ul> <p>CI &amp; testing:</p> <ul> <li>Add CI matrix for <code>with_tch</code>, <code>with_tokenizers</code>, and <code>safe_tensors</code> as already added.</li> <li>Add test suites that run cpu-only and feature gated tests.</li> <li>Add benchmark comparisons between CPU and GPU backends using <code>benches/</code>.</li> </ul> <p>Notes:</p> <ul> <li>Provide a simple CPU to GPU conversion path for Tensor storage \u2014 i.e., a <code>Tensor::to_device(device)</code> API that returns   a Tensor stored in a backend-specific memory region.</li> <li>Use <code>wgpu</code> and <code>cudarc</code> features behind optional Cargo features.</li> </ul>"},{"location":"bench_descriptions/","title":"Bench Descriptions \u2014 tensor_engine (Criterion benches)","text":"<p>This document explains the purpose and intent behind each Criterion bench in <code>benches/matmul_bench.rs</code>.</p> <p>General Notes</p> <ul> <li>Group names: benches are grouped into <code>matmul</code>, <code>ops</code>, <code>nn</code>, <code>training</code>, <code>batched_block_quant</code>, and a   <code>quantized_dequant_compare</code> sub-group. Group names show up in the Criterion output and help categorize microbenchmarks   by domain.</li> <li>CI gating: The repository uses an environment variable <code>CI_BENCH</code> to shorten test duration in CI. Set <code>CI_BENCH</code> to   any value to use shorter measurement times, smaller sample sizes, and shorter warmup times.</li> <li>Feature flags: Some benches are conditional on features such as <code>dtype_f16</code>, <code>dtype_bf16</code> and <code>openblas</code>. When running   local benches add those feature flags with <code>--features \"openblas,dtype_f16\"</code> as needed.</li> <li>Running a specific bench group: <code>cargo bench matmul</code>, <code>cargo bench ops</code>, <code>cargo bench nn</code>, <code>cargo bench training</code>,   <code>cargo bench batched_block_quant</code>; to run the whole bench suite, run <code>cargo bench</code>.</li> </ul> <p>Bench group: \"matmul\" Purpose: Measure core matrix multiply performance across several sizes, data types, and paths (float, quantized), plus autograd (forward+backward) impact.</p> <p>Key benches:</p> <ul> <li><code>matmul_{}x{}</code> (sizes: 10, 50, 100, 200): Measure the raw float matmul throughput for small to medium square   matrices \u2014 useful for micro-optimizing CPU matrix multiply or BLAS-backed matmul.</li> <li><code>matmul_forward_backward_{}x{}</code>: Measures the cost of forward + backward (autograd) for matmul at the same sizes; this   highlights autograd overhead and memory traffic of creating and backpropagating through computational graphs.</li> <li><code>quantized_matmul_{}x{}</code>: Measures the runtime of <code>QuantizedMatMul</code> when the right-hand side is stored as int8 with   scale. Use this to compare the quantized fast path vs float matmul and dequantization costs.</li> <li><code>matmul_f16_{}x{}</code> / <code>matmul_bf16_{}x{}</code> (feature-gated): Test matmul when using the f16/bf16 simulated dtypes; useful   to validate multi-precision path and any compile-time flags or vendor acceleration.</li> <li><code>matmul_{}x{}x{}</code> (rectangular case 64x128*128x64): Measures non-square matmul behavior (memory access, cache   effects).</li> <li>Large matmul sizes (512, 1024): Gate under <code>CI_BENCH</code> false; measure more realistic large-matrix performance and   compare int8 quantization to dequantized float path (the bench includes <code>dequantized_matmul_{}x{}</code> to measure   dequantization + float matmul). These are heavier tests; use them for local profiling or nightly runs.</li> </ul> <p>Bench group: \"quantized_dequant_compare\" Purpose: Direct microbenchmark comparing the quantized matmul runtime vs an explicit dequantize-then-multiply (float) path.</p> <p>Key benches:</p> <ul> <li><code>quantized_matmul_int8_128x128</code>: quantized matmul with 128x128 size.</li> <li><code>dequantized_matmul_128x128</code>: dequantize int8 storage to float and then perform row-major float matmul. Use this pair   to compare the speedup (or slowdown) from keeping the right-hand side int8 vs dequantizing it to float.</li> </ul> <p>Bench group: \"ops\" Purpose: Microbench elemental operators and small utilities (e.g., stack/concat) for throughput-focused operations and autograd overhead on small tensors.</p> <p>Key benches:</p> <ul> <li><code>add</code>, <code>mul</code>, <code>relu</code>, <code>sigmoid</code>, <code>tanh</code>, <code>log</code>, <code>pow</code>, <code>sum</code>, <code>mean</code>: Small-element ops that show per-element   computation and memory throughput.</li> <li><code>stack</code>, <code>concat</code>: Memory and copy performance; important for batching or preprocessing logic.</li> <li><code>softmax_axis1</code>, <code>log_softmax_axis1</code>: Softmax and log-softmax performance for 2D inputs; important for classification   and training workloads.</li> <li><code>ternary_forward</code>, <code>ternary_forward_backward</code>: Ternary quantization forward pass &amp; autograd loop (forward+backward) \u2014   helps validate quantization-friendly ops.</li> <li><code>add_forward_backward</code>: Autograd overhead for a small element-wise op.</li> </ul> <p>Bench group: \"nn\" Purpose: High-level neural network building blocks, including linear layers, normalization, dropout, convolutional layers (1D/2D/3D), pooling, attention layers and multi-head attention variants.</p> <p>Key benches &amp; rationale:</p> <ul> <li><code>linear_10_5</code>, <code>linear_5_1</code>: Forward pass throughput for small dense layers \u2014 baseline for small networks.</li> <li><code>linear_10_5_backward</code>: Backward pass overhead for linear layers (gradable gradients), important for training   performance assessment.</li> <li><code>layernorm_1x10</code>, and <code>layernorm_1x10_backward</code>: LayerNorm forward and backward cost which is often a small but   important per-token cost in transformers.</li> <li><code>dropout_training</code> vs <code>dropout_eval</code>: Compare runtime of Dropout in training and eval modes (eval should be identity   path).</li> <li><code>conv2d_3x64x64</code>, <code>conv2d_3x64x64_backward</code>: 2D convolution forward/backward \u2014 measure compute and memory effects for   convolutional layers.</li> <li><code>maxpool2d_3x64x64</code>: MaxPool forward throughput.</li> <li><code>conv1d_3x128</code>, <code>conv3d_3x8x32x32</code>, <code>depthwise_separable_conv2d_3x64x64</code>, <code>convtranspose2d_3x64x64</code>,   <code>avgpool2d_3x64x64</code>, <code>adaptive_avgpool2d_3x64x64</code>: Op-level benches for different convolution and pooling ops used in   vision and other workloads.</li> <li><code>absolute_positional_embedding_1x128x64</code>: Embedding forward cost, useful for transformer-like inputs.</li> <li><code>mha_forward_64_128</code>, <code>mha_forward_alibi_64_128</code>: MultiHeadAttention forward, with and without ALiBi, to measure   attentional compute; important for sequence model baselines.</li> <li>NL-OOB (mha forward with distance matrix) args through several sequence lengths and batch sizes: measure overhead of   non-local out-of-bound biases (distance-based attention) and scaling of memory/computation as seq_len and batch size   increase. Also includes backward variants to measure training costs.</li> <li><code>mha_forward_backward_64_128</code>, <code>mha_forward_backward_alibi_64_128</code>: MHA backward cost that includes gradient   propagation and KV state cost.</li> </ul> <p>Bench group: \"batched_block_quant\" Purpose: Batched matmul performance and block-wise quantization throughput.</p> <p>Key benches:</p> <ul> <li><code>batched_matmul_16_64_128_64</code>: Measures batched matmul throughput using an op-level BatchedMatMul. Useful for   production-style throughput where multiple sequences/batches compute matmul in bulk.</li> <li><code>block_quantized_matmul_512x512_block64</code>: (heavy, gated under CI_BENCH false) Splits the RHS into column blocks,   quantizes each block and performs repeated quantized matmul segments \u2014 this emulates blockwise quantization used by   many quantization schemes (e.g., AWQ/GPTQ) and measures block-chunked matmul throughput.</li> </ul> <p>Bench group: \"training\" Purpose: Measure training loop micro-benchmarks including forward, backward, optimizer steps, and data loading overhead.</p> <p>Key benches:</p> <ul> <li><code>training_step</code>: A small model forward pass, compute loss (MSE), \u2014 bench that measures forward-only training   throughput.</li> <li><code>training_step_backward</code>: Forward + backward pass \u2014 measures gradient computation overhead and shows end-to-end step   costs without optimizer updates.</li> <li>Optimizer benches: <code>sgd_step</code>, <code>adam_step</code>: Bench the optimizer <code>step</code> + <code>zero_grad</code> for a small model. Particularly   useful to compare optimizer overhead on parameter updates and memory usage patterns.</li> <li>DataLoader bench: <code>dataloader_shuffle</code>, <code>dataloader_next_batch</code> \u2014 measure shuffle cost and iteration cost across small   datasets to ensure batching and shuffle logic don't become bottlenecks.</li> </ul> <p>How to use these benches:</p> <ul> <li>ShortCI mode: <code>CI_BENCH=1 cargo bench matmul</code> runs shorter measurement times to make CI pass faster; useful when   verifying code changes quickly.</li> <li>Local profile: <code>cargo bench</code> or <code>cargo bench matmul</code> without <code>CI_BENCH</code> runs longer measurements for a stable result \u2014   recommended for profiling and performance tuning.</li> <li>Test with features: include <code>--features \"openblas multi_precision dtype_f16\"</code> to run with vendor libs or simulated   dtypes.</li> </ul> <p>Interpretation tips:</p> <ul> <li>Compare <code>quantized_matmul_*</code> vs <code>dequantized_matmul_*</code> to see if keeping compressed int8 data yields runtime   improvement on your target hardware. Also check the memory usage \u2014 dequantizing increases memory and can hurt cache   locality.</li> <li>Matmul forward vs forward+backward shows autograd/graph overhead; for training systems, forward+backward cost is often   far more relevant than pure forward.</li> <li>Pay attention to <code>mha_</code> group results and NL-OOB tests to understand how attention scales with sequence length and   batch size \u2014 most transformer tuning happens here.</li> <li>Use <code>benches/</code> as a regression test: add new bench or compare commit results with prior runs to identify regressions   or improvements.</li> </ul> <p>Example run commands (Windows / Linux / macOS):</p> <p>Quick (CI-like short measurements):</p> <pre><code># Run only the matmul benches quickly (CI_BENCH short run)\n$env:CI_BENCH = 1\ncargo bench matmul --features \"openblas\"  # or omit features as required\n</code></pre> <p>Full suite (local profiling):</p> <pre><code># Run full criteria benches locally (longer execution time, better statistics)\ncargo bench --features \"openblas multi_precision dtype_f16\"\n</code></pre> <p>Reproduce a specific bench (e.g., quantized vs dequantize comparison):</p> <pre><code># Quantized vs dequantized microbench for 128x128\ncargo bench quantized_dequant_compare\n</code></pre> <p>Notes for bench authors and contributors</p> <ul> <li>Consistent RNG seeds ensure repeatable inputs and stable comparisons across runs.</li> <li>Use <code>CI_BENCH</code> for CI-friendly runs to keep jobs fast; separate nightly/profile runs for full statistics.</li> <li>If testing vendor acceleration or dtype differences, gate them under feature flags and document results as part of the   PR.</li> <li>Add microbench notes in <code>docs/bench_descriptions.md</code> whenever adding new benches so others can easily understand the   rationale.</li> </ul> <p>If you'd like, I can:</p> <ul> <li>Add a script that runs selected benches and logs results to <code>bench_reports/</code> (with timestamped CSVs), or</li> <li>Create a simple CI job that runs a subset of performance-critical benches nightly and stores the artifacts.</li> </ul> <p>End of bench explanation document.</p>"},{"location":"kronos_integration/","title":"Kronos Format Integration","text":"<p>This doc explains how the <code>kronos</code> format is supported in <code>tensor_engine</code>.</p> <ul> <li>The Kronos format is encoded in SafeTensors archives and includes a metadata marker <code>__kronos_marker__</code> with value   <code>4B524F4E</code> (hex for 'KRON') or <code>format: Kronos</code>.</li> <li>Use <code>tensor_engine::io::safetensors_loader::apply_kronos_bytes_to_module_bytes</code> to load a Kronos file into a   <code>MultimodalLLM</code> instance.</li> <li>The function maps Kronos keys such as <code>vision_encoder.*</code>, <code>text_embedding.weight</code>, <code>projector.*</code>,   <code>decoder_blocks.layers.*</code>, and <code>head.*</code> into the module fields.</li> </ul> <p>See <code>kronos-modal-format.md</code> for the full schema and recommended mapping semantics.</p>"},{"location":"quickstart/","title":"Quickstart \u2014 Tensor Engine","text":"<p>This doc helps contributors run and test the project locally.</p> <p>Prerequisites</p> <ul> <li>Rust toolchain (stable/1.70+ recommended)</li> <li>Python 3.11 (if building Python bindings) and maturin (optional)</li> <li>On Windows: Visual Studio C++ toolchain</li> <li>Optional: OpenBLAS binaries (set OPENBLAS_DIR or use <code>scripts/setup_dev_repo.ps1</code>)</li> </ul> <p>Build &amp; Run</p> <ul> <li> <p>Build library</p> <ul> <li><code>cargo build</code> (default features)</li> <li><code>cargo build --features \"openblas,python_bindings\"</code> (add features as needed)</li> </ul> </li> <li> <p>Run tests</p> <ul> <li><code>cargo test</code></li> <li><code>cargo test --features \"openblas,with_tch\"</code> (if you have those toolchains installed)</li> </ul> </li> <li> <p>Python bindings (dev)</p> <ul> <li>Install <code>maturin</code> in your environment, e.g. <code>pip install maturin</code></li> <li>Build &amp; install dev wheel (editable): <code>maturin develop --release --bindings pyo3 --features python_bindings</code></li> <li>Or build a release wheel: <code>maturin build --release --bindings pyo3 --features python_bindings</code></li> <li>Run the Python smoke test (in the venv where the package is installed):   <pre><code>python tests/python_smoke_test.py\n</code></pre></li> </ul> </li> </ul> <p>Examples</p> <ul> <li>Rust examples:<ul> <li><code>cargo run --example blas_check --features \"openblas\"</code> (Unix only)</li> <li><code>cargo run --example sample_diffusion</code> (if features/inputs satisfied)</li> </ul> </li> <li>Python examples:<ul> <li><code>python examples/linear_regression.py</code></li> <li><code>python examples/load_model.py</code> (requires Python bindings and tokenizers if using them)</li> </ul> </li> </ul> <p>Notes</p> <ul> <li>When building on Windows with <code>openblas</code>, run <code>scripts/setup_dev_repo.ps1</code> to configure <code>OPENBLAS_DIR</code> and add DLLs to   PATH.</li> <li>Many tests and examples require optional features (OpenBLAS, Python bindings, tch); run <code>cargo test --all-features</code>   only in CI planned environments.</li> <li>Optional: <code>--features with_tch</code> requires a prebuilt libtorch or installed Python torch wheel; on Windows, follow CI   approach to download shared libtorch and set <code>LIBTORCH</code> and PATH accordingly.</li> </ul> <p>If you want a minimal quickstart for a new dev environment, I can add a <code>scripts/setup_dev_repo.ps1</code> walkthrough and a <code>docs/index.md</code> linking to this quickstart.</p>"},{"location":"resampling/","title":"Resampling in WavDataLoader","text":"<p>This document explains resampling choices available in <code>WavDataLoader</code> and how to enable high-quality resampling.</p> <p>Options:</p> <ul> <li><code>resample</code> flag in <code>WavDataLoader::new(dir, sample_rate, chunk_len, batch_size, resample)</code>: When true, WAV files with   mismatched sample rates will be resampled to the requested sample rate.</li> <li>By default, <code>resample</code> uses a simple linear resampler implemented in <code>src/io/dataloader.rs::resample_linear</code> as a fast   fallback.</li> <li>When the library is compiled with the <code>audio</code> feature, and the <code>rubato</code> feature is also enabled (e.g.   <code>--features \"audio openblas\"</code>), <code>WavDataLoader</code> will use <code>rubato</code>-based high-quality resampling via   <code>resample_high_quality</code>. This resampler implements band-limited sinc interpolation for good audio quality.</li> </ul> <p>How to enable:</p> <ul> <li>Build with <code>audio</code> feature: <code>cargo build --features audio</code>.</li> <li>For high-quality (rubato), use: <code>cargo build --features \"audio\"</code> (rubato is included under <code>audio</code> feature as an   optional dep).</li> </ul> <p>Notes &amp; recommendations:</p> <ul> <li>High-quality resampling results in better audio fidelity but increases CPU cost \u2014 set <code>resample=true</code> only when   needed.</li> <li>For large training datasets, preprocess resampling offline or use a streaming resampling strategy in the data   pipeline.</li> </ul> <p>See <code>src/io/dataloader.rs</code> for implementation details and <code>examples/train_codec.rs</code> for the training example using resampling.</p>"},{"location":"rvq_ema/","title":"RVQ EMA Updates &amp; Scheduling","text":"<p>This document outlines recommended settings and behaviors for the RVQ <code>update_ema()</code> method.</p> <p>Key behaviors:</p> <ul> <li><code>RVQ::update_ema(inputs, indices, decay)</code> computes per-level residual statistics and updates codebooks using an   unbiased EMA using per-code <code>ema_counts</code>.</li> <li><code>set_ema_update_every(n)</code> will only perform the update every <code>n</code> calls to reduce CPU/GPU overhead. Default is <code>1</code> (   every call).</li> <li><code>set_reinit_empty_codes(true)</code> will reinitialize empty codes (those with zero recent counts) from random residual   vectors to avoid dead codes.</li> </ul> <p>Recommended values for training:</p> <ul> <li><code>decay = 0.999</code> (slow moving average \u2014 common schedule used in vector quantization like VQ-VAE and RVQ).</li> <li><code>ema_update_every = 1</code> for small batch sizes, <code>ema_update_every = 32</code> for very large batch training to amortize cost.</li> <li><code>reinit_empty_codes = true</code> when training from scratch to avoid dead codes; once training stabilizes, you can turn it   off.</li> </ul> <p>Scheduling examples:</p> <ul> <li>Update per batch (default): <code>rvq.set_ema_update_every(1); rvq.update_ema(&amp;inputs, &amp;indices, 0.999);</code></li> <li>Update every 16 batches to reduce overhead: <code>rvq.set_ema_update_every(16);</code> \u2014 call <code>update_ema()</code> every training step   as usual; internal scheduling will only run on the correct step.</li> </ul> <p>Notes:</p> <ul> <li>For distributed training, ensure <code>ema_counts</code> and codebooks are synchronized across replicas or perform server-side   updates after accumulation.</li> <li>For empty cluster reinitialization, the code picks a random residual to reseed the codebook entry and sets an initial   <code>ema_count</code> small positive value so it will participate later.</li> </ul> <p>See <code>src/nn/quantization.rs</code> for implementation details.</p>"},{"location":"test_descriptions/","title":"Test Descriptions \u2014 tensor_engine","text":"<p>This document explains the purpose of unit and integration tests under <code>tests/</code> and the <code>src/nn/tests/</code> folder. Use this as a quick reference to understand what each test validates and why it exists.</p> <p>General notes</p> <ul> <li>Feature-gated tests: Many tests are conditionally compiled behind feature flags. See <code>#[cfg(feature = \"openblas\")]</code> (   and similar) attributes in the files. Examples include <code>with_tch</code>, <code>safe_tensors</code>, and dtype feature flags like   <code>dtype_f16</code>.</li> <li>TorchScript fixtures: Some TorchScript loader tests use checked-in <code>.pt</code> files in <code>tests/assets/</code>. Where fixtures are   absent, tests try to decode a <code>.b64</code> file or generate fixtures using <code>python</code> and <code>torch</code> at runtime (skipping if   Python+torch isn't available).</li> <li>Numeric/grad checks: Several tests implement finite difference numeric gradients to validate analytic autograd   computations.</li> <li>CI behavior: CI can be configured via environment variables such as <code>CI_BENCH</code>/<code>CI_TEST</code> and feature flags recommended   for faster CI runs. See <code>.github/workflows/ci.yml</code>.</li> </ul> <p>Tests in <code>tests/</code></p> <ul> <li> <p><code>tokenizer_test.rs</code></p> <ul> <li>test_tokenizer_load_and_encode: Verify that tokenizer loader fails gracefully when a tokenizer file does not   exist. Ensures the wrapper handles file errors gracefully.</li> <li>Feature: <code>with_tokenizers</code>.</li> </ul> </li> <li> <p><code>state_dict_default.rs</code></p> <ul> <li>test_default_load_state_dict_applies_named_parameters: Verify that the module <code>Sequential</code> default state_dict   loader applies parameters from a <code>HashMap&lt;String, Tensor&gt;</code> using <code>named_parameters</code> keys. Ensures loading and key   binding works for common module naming patterns.</li> </ul> </li> <li> <p><code>safetensors_loader_full.rs</code> (feature: <code>safe_tensors</code>, <code>multi_precision</code>)</p> <ul> <li>test_safetensors_full_state_dict_load: Create a safetensors fixture and load it into <code>MultiHeadAttention</code> module;   confirm weights of <code>linear_q</code> are updated correctly. Tests the safetensors loader end-to-end and multi-precision   parsing.</li> </ul> </li> <li> <p><code>safetensors_test.rs</code> (feature: <code>safe_tensors</code> and <code>multi_precision</code>)</p> <ul> <li>test_safetensors_loader_invalid_bytes: Ensures invalid bytes cause an error during safetensor load.</li> <li>test_safetensors_parse_f32_and_f16: Confirms safetensors parsing works for F32, F16, and BF16 encodings.</li> </ul> </li> <li> <p><code>quantized_matmul_test.rs</code></p> <ul> <li>test_quantized_matmul_basic: Ensures the quantized matmul path runs and returns the expected shape for a simple   input. This is a shape &amp; basic execution check for the int8 path.</li> </ul> </li> <li> <p><code>pytorch_loader_test.rs</code> (feature: <code>with_tch</code>)</p> <ul> <li>missing_file_returns_error: Check loader returns error with helpful message when file is missing.</li> <li>cmodule_extraction_via_python_jit: Tests dispatcher that extracts named_parameters &amp; state_dict from a TorchScript   CModule created via traced Python code, using baked fixture or generated file. Skips if Python+torch isn't   available.</li> <li>cmodule_nested_state_dict_extraction: Ensures loader can parse nested state_dict results from TorchScript (when   state_dict returns a dict with nested keys).</li> <li>cmodule_state_dict_list_pairs_extraction: Tests the loader handles <code>state_dict</code> returning a list of (name,tensor)   pairs.</li> <li>cmodule_state_dict_hashmap_extraction: Test the loader handles a <code>state_dict</code> that aliases keys (hashmap) and maps   them to parameter names.</li> </ul> </li> <li> <p><code>optimizer_test.rs</code></p> <ul> <li>test_linear_forward / test_sequential: Basic shape checks for <code>Linear</code> and <code>Sequential</code> modules.</li> <li>test_sgd_step, test_sgd_momentum_behaviour: Validate step semantics of SGD with and without momentum and parameter   updates.</li> <li>test_zero_grad / test_optimizer_zero_grad: Ensure <code>zero_grad()</code> works on single <code>Tensor</code> and Optimizer-level   zeroing.</li> <li>test_sgd_cast_params_dtype_changes: Tests casting to F8 changes dtype and affects stored values.</li> <li>test_adamw_weight_decay_reduces_param_more_than_adam: Confirms <code>AdamW</code> weight decay effect vs <code>Adam</code> when weight   decay is non-zero.</li> <li>test_adamw_matches_adam_with_zero_weight_decay: <code>AdamW</code> with zero weight decay matches Adam behavior.</li> <li>test_linear_warmup_scheduler, test_cosine_annealing_scheduler, test_rmsprop_step: Basic checks for scheduler   outputs and RMSProp step behavior.</li> </ul> </li> <li> <p><code>nn_extra_test.rs</code></p> <ul> <li>test_rnncell_forward_backward: RNNCell forward/backward shapes &amp; gradients.</li> <li>test_lstmcell_forward_backward: LSTMCell forward/backward shapes &amp; gradient checks.</li> <li>test_attention_forward_shape_and_grad: Self-Attention forward/backward shapes &amp; gradient propagation.</li> <li>test_transformer_forward_shape: TransformerBlock shape check.</li> <li>test_convblock_and_gan_forward: ConvBlock forward shape and simple generator/discriminator forward shape checks.</li> </ul> </li> <li> <p><code>nl_oob_test.rs</code> (NL-OOB = Non-Local / Out-of-Bounds attention biasing)</p> <ul> <li>test_nl_oob_forward_affects_logits: Verify using NL-OOB distance-based bias alters attention outputs when provided   an appropriately configured <code>MultiHeadAttention</code> with slopes and bias function.</li> </ul> </li> <li> <p><code>new_ops_test.rs</code></p> <ul> <li>test_rmsnorm_forward_backward_shapes: Ensure RMSNorm forward shape &amp; backward gradient are computed.</li> <li>test_swiglu_forward_backward_shapes: Ensure SwiGLU forward/backward correctness and shapes.</li> <li>test_embedding_lookup_forward_backward: Embedding lookup forward shape and gradient.</li> <li>test_kvcache_append: KV cache append op shape, result, and gradient flow.</li> <li>test_numeric_gradient_*: Finite-difference numerical gradient checks for new ops (RMSNorm, SwiGLU) to detect   autograd correctness.</li> </ul> </li> <li> <p><code>dtype_tests.rs</code></p> <ul> <li>test_astype_f8_roundtrip_and_dtype: Verify dtype cast to F8 and shape preservation works and dtype is set.</li> <li>test_astype_f16_roundtrip_and_loss: (feature gated <code>dtype_f16</code>) Tests F16 conversion shape and numerical loss vs   F32.</li> <li>test_astype_bf16_roundtrip_and_loss: (feature gated <code>dtype_bf16</code>) BF16 conversion and round-trip check.</li> </ul> </li> <li> <p><code>blas_matmul_test.rs</code></p> <ul> <li>test_blas_matmul_matches_ndarray_for_various_shapes: Compare numeric matmul result of our BLAS-backed matmul   against <code>ndarray</code> dot across shapes; verifies numerical parity across shapes.</li> </ul> </li> <li> <p><code>autograd_test.rs</code> (large file)</p> <ul> <li>A thorough set of tests for core autograd correctness, including:<ul> <li>Scalar arithmetic ops (add, mul, sub, div, pow, exp, tanh, sigmoid) \u2014 forward values and backward gradients.</li> <li>Element reductions (sum, mean, max, min) \u2014 forward and backward semantics and tie behavior (splitting   gradients across ties).</li> <li>Matmul forward/backward gradients and shapes.</li> <li>Activation functions: relu, gelu; shape tests and numerical grad checks</li> <li>Ternary operator tests (ternary quant) \u2014 forward/grad behavior.</li> <li>Broadcast behavior and gradient reduction across broadcasted dims for add &amp; mul.</li> <li>Numeric gradient tests for add, mul, broadcast add/mul, pow, sigmoid, gelu, exp \u2014 finite-difference gradient   checks.</li> <li>MSE/cross-entropy/NLL Loss gradient checks: validate loss gradients are computed as per analytical formulae.</li> <li>DataLoader shuffle/next_batch semantics \u2014 random shuffle and batch iteration.</li> <li>Dropout forward/backward in training and evaluation modes.</li> <li>Convolution and pooling ops (conv2d, conv3d, conv1d, depthwise separable conv, conv transpose)   forward/backward correctness and gradient shapes.</li> <li>MaxPool/AvgPool forward/backward checks with expected gradient values in pooled windows.</li> <li>LayerNorm forward and backward numeric checks.</li> </ul> </li> </ul> </li> <li> <p><code>attention_variants_test.rs</code></p> <ul> <li>test_flash_ref_and_chunked_match_baseline: Validate that FlashRef and Chunked attention produce numerically   similar outputs to baseline (within tolerance), ensuring alternate kernels are functionally equivalent.</li> </ul> </li> <li> <p><code>attention_grad_parity_test.rs</code></p> <ul> <li>test_attention_grad_parity_flashref and test_attention_grad_parity_chunked: Compare gradient parity between the   baseline attention implementation and either FlashRef or Chunked variants, ensuring their gradients (backprop) are   numerically close to the baseline.</li> </ul> </li> </ul> <p>Tests in <code>src/nn/tests/</code> (module-level tests)</p> <ul> <li> <p><code>transformer_tests.rs</code> (transformer &amp; NL-OOB tests)</p> <ul> <li>transformer_block_forward_shape: Basic shape check for <code>TransformerBlock</code>.</li> <li>mha_forward_with_distance_applies_penalty: Ensure the NL-OOB <code>forward_with_distance</code> produces different outputs   than base attention.</li> <li>mha_slopes_are_learnable_and_receive_grad: Ensure slopes parameter exists and receives gradients.</li> </ul> </li> <li> <p><code>transformer_rope_gqa_tests.rs</code></p> <ul> <li>transformer_block_rope_and_gqa_shapes: Validate TransformerBlock with RoPE and Grouped Query Attention (GQA)   combinations preserve input-output shapes.</li> </ul> </li> <li> <p><code>transformer_nl_oob_tests.rs</code></p> <ul> <li>A collection of NL-OOB focused tests:<ul> <li>mha_forward_with_distance_shapes_and_slopes_present (detect slopes and parameter names)</li> <li>mha_forward_with_distance_batch_and_gaussian (batchwise distances)</li> <li>mha_forward_with_distance_mismatched_batch_returns_input (test mismatch behavior where distance batch shape   mismatched -&gt; identity return)</li> <li>transformer_block_forward_with_distance_integrates_nl_oob (ensure TransformerBlock integrates NL-OOB MHA   correctly)</li> <li>transformer_block_builder_with_nl_oob_works (module builder creates named slopes param)</li> <li>load_state_dict_sets_nl_oob_config_from_state (loader sets slope config and slopes and requires grad   correctly)</li> <li>slopes_receive_grad_on_backward (slopes receive grad backprop)</li> <li>mha_forward_with_distance_2d_phi_broadcasts_to_batch (2D distance broadcast semantics)</li> </ul> </li> </ul> </li> <li> <p><code>flatten_tests.rs</code> (small module tests)</p> <ul> <li>flatten_works_on_4d: ensure <code>Flatten</code> reduces N-D to 2-D for conv outputs.</li> <li>flatten_integrates_with_sequential: Validate <code>Flatten</code> integrates with <code>Sequential</code> and subsequent layers.</li> </ul> </li> </ul> <p>How to run the test suite</p> <ul> <li>Basic unit tests (default features):</li> </ul> <pre><code>cargo test\n</code></pre> <ul> <li>Run a specific test file or test name (useful for debugging):</li> </ul> <pre><code># Run the test file with Rust's `--test` harness name\ncargo test --test quantized_matmul_test\n# or a specific test name\ncargo test test_quantized_matmul_basic\n</code></pre> <ul> <li>Run tests that require optional features (e.g., <code>with_tch</code>, <code>safe_tensors</code>):</li> </ul> <pre><code>cargo test --features \"with_tch\"  # for TorchScript loader tests\ncargo test --features \"safe_tensors multi_precision\"  # safe tensor parsing tests\n</code></pre> <ul> <li>Notes for developers<ul> <li>RNG seeds are fixed in tests to ensure deterministic, reproducible results across runs.</li> <li>Some tests are intentionally skipped if a feature or runtime dependency (Python/Torch) isn't present. This keeps   CI stable.</li> <li>Many tests run heavy numeric checks \u2014 use short or subset runs while iterating on code.</li> </ul> </li> </ul> <p>If you want, I can add:</p> <ul> <li>A single <code>scripts/run_tests.sh</code> or <code>scripts/run_tests.ps1</code> that runs common test subsets (fast, full, feature-gated).</li> <li>A CI matrix example that runs extended tests nightly and marks heavy tests as <code>allow_failures: false</code> for catching   regressions.</li> </ul> <p>End of <code>docs/test_descriptions.md</code>.</p>"},{"location":"vlm_guide/","title":"Vision-Language Model (VLM) Guide","text":"<p>This guide documents how to use the <code>MultimodalLLM</code> and associated IO utilities in <code>tensor-engine</code>.</p>"},{"location":"vlm_guide/#prefill-decode-usage","title":"Prefill / Decode Usage","text":"<p>To optimize multimodal decoding, the <code>MultimodalLLM</code> supports a prefill phase followed by decode steps:</p> <ul> <li><code>prefill(images, input_ids=None)</code> computes and caches the image features and optional text prefix embeddings in a   <code>ModalMemoryContext</code>.</li> <li><code>decode_step(memory, new_input_ids)</code> appends <code>new_input_ids</code> token embeddings into the memory context and returns   logits and an updated <code>ModalMemoryContext</code> for further decoding.</li> </ul> <p>This allows efficient autoregressive generation without recomputing image features at each token step.</p>"},{"location":"vlm_guide/#imagetextdataloader","title":"ImageTextDataLoader","text":"<p><code>ImageTextDataLoader</code> loads image/caption pairs from a manifest file where each line is:</p> <pre><code>&lt;image_path&gt;\\t&lt;caption&gt;\n</code></pre> <p>Usage example (Rust):</p> <pre><code>use tensor_engine::io::image_text_dataloader::ImageTextDataLoader;\nlet loader = ImageTextDataLoader::new_from_manifest(\"manifest.txt\", (224,224), 8, true, true)?;\nlet (images, captions) = loader.load_batch(0)?;\n</code></pre> <p>For Python, use the <code>ImageTextDataLoader</code> wrapper when compiled with <code>python_bindings</code> and <code>vision</code> features:</p> <pre><code>from tensor_engine import ImageTextDataLoader\nloader = ImageTextDataLoader('manifest.txt', 224, 224, 8, True, True)\nimages, captions = loader.load_batch(0)\n</code></pre>"},{"location":"vlm_guide/#tokenizer","title":"Tokenizer","text":"<p>If <code>with_tokenizers</code> and <code>python_bindings</code> are enabled, <code>PyTokenizer</code> exposes <code>encode(text)</code> and <code>decode(ids)</code>.</p>"},{"location":"vlm_guide/#examples","title":"Examples","text":"<p>See <code>examples/generate_llava.py</code> and <code>examples/train_llava.py</code> for end-to-end training and inference.</p>"}]}